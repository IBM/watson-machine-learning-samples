{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9034efb-4104-4176-950e-0de875eb07f5"
   },
   "source": [
    "![](https://github.ibm.com/WML/watson-machine-learning-samples/blob/dev/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png?raw=true \"title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use lm-evaluation-harness and own benchmarking data with watsonx.ai foundation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00639b8e-fe72-4a3e-bea3-d40fdc55fb7d"
   },
   "source": [
    "This notebook contains the steps and code to demonstrate usage of `lm-evaluation-harness` (also called `lm-eval`) package with `ibm_watsonx_ai` and `watsonx_llm` language model.   \n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "078d559c-58fc-4899-803c-cc3a58c4038d"
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are: \n",
    "1. Setting up `lm-evaluation-harness` and `ibm_watsonx_ai`\n",
    "2. Basic `lm-evaluation-harness` usage with available tasks\n",
    "3. Preparing custom tasks and setting up local datasets\n",
    "4. Calling `lm-evaluation-harness` with locally prepared tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6bd4d32-b6d7-4056-9138-a41665c3fd60"
   },
   "source": [
    "## Table of contents\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "   - [How to install lm-evaluation-harness - two ways](#install-lm-eval)\n",
    "   - [Install lm-evaluation-harness and ibm_watsonx_ai](#install-packages)\n",
    "   - [Validate installation](#validate-install)\n",
    "3. [Setting up necessary IBM watsonx credentials](#credentials)\n",
    "   - [Working with projects](#projects)\n",
    "   - [Export watsonx variables to be used by lm-evaluation-harness](#export-variables)\n",
    "4. [Basic lm-evaluation-harness usage](#usage)\n",
    "5. [Preparing own data for benchmarking](#prepare-own-data)\n",
    "   - [Prepare an APIClient instance](#prepare-client)\n",
    "   - [Prepare data assets](#prepare-assets)\n",
    "   - [Download datasets](#download-files)\n",
    "   - [Validate download](#validate-download)\n",
    "   - [Sample YAML task syntax](#yaml-task)\n",
    "   - [Save custom task to file](#save-task)\n",
    "6. [Run lm-evaluation-harness benchmarks with local data](#benchmarks-local)\n",
    "7. [Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4316b92d-682b-408c-a8b7-8ea8bc3c5179"
   },
   "source": [
    "## Prerequisites <a name=\"prerequisites\"></a>\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "- Create a [watsonx.ai Runtime](https://cloud.ibm.com/catalog/services/watson-machine-learning) instance (a free plan is offered and information about how to create the instance can be found [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp)).\n",
    "- Create a [Cloud Object Storage (COS) instance](https://console.bluemix.net/catalog/infrastructure/cloud-object-storage) (a lite plan is offered and information about how to order storage can be found [here](https://cloud.ibm.com/docs/cloud-object-storage/basics/order-storage.html#order-storage)).\n",
    "  \n",
    "__Note: When using Watson Studio, you already have a COS instance associated with the project you are running the notebook in.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to install lm-evaluation-harness - two ways <a name=\"install-lm-eval\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "698a888c-e4bb-4f4d-aae8-84dd2cfa5d64"
   },
   "source": [
    "`lm-evaluation-harness` is a unified framework to test generative language models on a large number of different evaluation tasks. For more info and the source code, check out its [GitHub repository](https://github.com/EleutherAI/lm-evaluation-harness/tree/main)\n",
    "\n",
    "1. Package installation - to use as is: \n",
    "    ```\n",
    "    !pip install lm-eval | tail -n 1\n",
    "    ```\n",
    "\n",
    "2. Local installation - for debugging purposes:\n",
    "\n",
    "    ```\n",
    "    git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "    cd lm-evaluation-harness\n",
    "    pip install -e .\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install ibm_watsonx_ai and lm-evaluation-harness package from pip <a name=\"install-packages\"></a>\n",
    "\n",
    "__Note__: \n",
    "- `ibm-watsonx-ai` documentation can be found [here](https://ibm.github.io/watsonx-ai-python-sdk/index.html).\n",
    "- `lm-evaluation-harness` documentation can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5025324a-7005-450a-aa9a-9922123d20e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm_watsonx_ai) (1.16.0)\n",
      "Successfully installed DataProperty-1.1.0 accelerate-1.2.1 chardet-5.2.0 colorama-0.4.6 datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 huggingface-hub-0.27.1 jsonlines-4.0.0 lm-eval-0.4.7 mbstrdecoder-1.1.3 more_itertools-10.6.0 multiprocess-0.70.16 pathvalidate-3.2.3 peft-0.14.0 portalocker-3.1.1 pybind11-2.13.6 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 safetensors-0.5.2 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tokenizers-0.21.0 tqdm-multiprocess-0.0.11 transformers-4.48.0 typepy-1.3.4 word2number-1.1 xxhash-3.5.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ibm_watsonx_ai | tail -n 1 \n",
    "!pip install lm-eval | tail -n 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01468868-b35a-40cb-9336-a800e4be2fa2"
   },
   "source": [
    "- `wget` is used only if you are planning to download datasets directly from HuggingFace. If you already have them stored locally or as data assets on Cloud, skip the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "93f49df3-4b35-40e9-914c-c3ba13d21eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "411a5800-931b-4d24-8c36-a88269cf70c3"
   },
   "source": [
    "#### Validate installation <a name=\"validate-install\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "93e3efd1-3e7b-4a59-950e-38383e5b159b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibm_watsonx_ai                          1.2.1\n",
      "lm_eval                                 0.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep ibm_watsonx_ai\n",
    "!pip list | grep lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up necessary IBM watsonx credentials <a name=\"credentials\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required credentials: \n",
    "- IBM Cloud API key,\n",
    "- IBM Cloud URL\n",
    "- IBM Cloud Project ID\n",
    "\n",
    "Authenticate the Watson Machine Learning service on IBM Cloud. You need to provide Cloud API key and location.\n",
    "\n",
    "Tip: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the __API Keys__ section, and click __Create an IBM Cloud API key__. Give your key a name and click __Create__, then copy the created key and paste it below. You can also get a service specific url by going to the [Endpoint URLs section of the watsonx.ai Runtime](https://cloud.ibm.com/apidocs/machine-learning) docs. You can check your instance location in your [watsonx.ai Runtime](https://cloud.ibm.com/catalog/services/watson-machine-learning) instance details.\n",
    "\n",
    "You can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve the instance location.\n",
    "\n",
    "```\n",
    "ibmcloud login --apikey API_KEY -a https://cloud.ibm.com\n",
    "ibmcloud resource service-instance WML_INSTANCE_NAME\n",
    "```\n",
    "\n",
    "NOTE: You can also get a service specific apikey by going to the [Service IDs section of the Cloud Console](https://cloud.ibm.com/iam/serviceids). From that page, click __Create__, and then copy the created key and paste it in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4278b53a-4e92-4de0-ae16-9950fdb5d0d1"
   },
   "source": [
    "Import widely used modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "14568225-0b99-4430-8684-740c3464b1a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74e34cd7-dba7-47dd-bcd7-7e023c8782f8"
   },
   "source": [
    "__Action__: Enter your `api_key` in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7fb3062d-ae42-450c-b407-7e60e92091f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your api key (hit enter):  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "api_key = getpass.getpass(\"Please enter your api key (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8bfb084-79b6-471d-b1f0-595ccf5edd69"
   },
   "source": [
    "__Action__: Enter your `location` in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d799446-9a41-4926-aba4-eade711537d4"
   },
   "outputs": [],
   "source": [
    "location = \"INSERT YOUR LOCATION HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28917545-dec7-408e-a27d-cf38f9000e06"
   },
   "source": [
    "If you are running this notebook on Cloud, you can access the `location` via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "525becc3-f5f9-43d7-a8ba-c85fcdcf3143"
   },
   "outputs": [],
   "source": [
    "location = os.environ.get(\"RUNTIME_ENV_REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8405371b-a233-4544-8a3f-a36060854588"
   },
   "outputs": [],
   "source": [
    "url = f\"https://{location}.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2d06fbb-6cb5-45ee-828f-86ea661963a9"
   },
   "source": [
    "## Working with projects <a name=\"projects\"></a>\n",
    "\n",
    "You need to create a project that will be used for your work. If you do not have a space, you can use [Projects Dashboard](https://dataplatform.cloud.ibm.com/wx/home?context=wx) to create one.\n",
    "\n",
    "- Click __Create a new project__\n",
    "- Provide a name\n",
    "- Select Cloud Object Storage\n",
    "- Select Watson Machine Learning instance and press __Create__\n",
    "- Copy `project_id` and paste it below\n",
    "\n",
    "__Action__: Assign project ID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2ad1b98-4dcb-4585-8bfb-56f558fa70e8"
   },
   "outputs": [],
   "source": [
    "project_id = \"INSERT YOUR PROJECT ID HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0596b06b-4b82-40ce-92f8-e47316dcaec7"
   },
   "source": [
    "If you are running this notebook on Cloud, you can access the `project_id` via: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ac7f4a6d-ebbb-409e-baa6-ab721f5183d6"
   },
   "outputs": [],
   "source": [
    "project_id = os.environ.get(\"PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03c6e761-a248-4f9f-bff0-60c921ad2ffa"
   },
   "source": [
    "## Export watsonx variables to be used by lm-evaluation-harness <a name=\"export-variables\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e3e84905-c10d-4426-b20e-3387463598aa"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WATSONX_API_KEY\"] = api_key\n",
    "os.environ[\"WATSONX_URL\"] = url\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "745fac85-275e-4540-8078-4a0b5bdf629a"
   },
   "source": [
    "## Basic `lm-evaluation-harness` usage <a name=\"usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad906a79-6fe6-4c70-b7ed-38e74dfd3221"
   },
   "source": [
    "Basic `lm-evaluation-harness` syntax requires providing: \n",
    "- model\n",
    "- specific model_id\n",
    "- task name\n",
    "\n",
    "```\n",
    "!lm_eval \\\n",
    "--model [model] \\\n",
    "--model_args model_id=[model_id] \\\n",
    "--limit 10 \\\n",
    "--tasks [task_name] \\\n",
    "```\n",
    "\n",
    "--limit 10 is used to evaluate only 10 records. \n",
    "\n",
    "In order to get more info about possible arguments, use the command: \n",
    "\n",
    "```\n",
    "!lm-eval -h\n",
    "```\n",
    "\n",
    "Sample calling using the `watsonx_llm` and an available `gsm8k` task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "b94c9c53-bc5d-42f3-9670-cbe034c4c6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-17:14:01:06,733 INFO     [client.py:443] Client successfully initialized\n",
      "2025-01-17:14:01:07,594 INFO     [wml_resource.py:112] Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-01-10&project_id=7e8b59ba-2610-4a29-9d90-dc02483ed5f4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "README.md: 100%|███████████████████████████| 7.94k/7.94k [00:00<00:00, 53.2MB/s]\n",
      "train-00000-of-00001.parquet: 100%|████████| 2.31M/2.31M [00:00<00:00, 2.55MB/s]\n",
      "test-00000-of-00001.parquet: 100%|███████████| 419k/419k [00:00<00:00, 5.43MB/s]\n",
      "Generating train split: 100%|████| 7473/7473 [00:00<00:00, 235820.14 examples/s]\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 262928.90 examples/s]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 294.85it/s]\n",
      "Running generate_until function ...:   0%|               | 0/10 [00:00<?, ?it/s]2025-01-17:14:01:24,661 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:24,662 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  10%|▋      | 1/10 [00:03<00:31,  3.48s/it]2025-01-17:14:01:28,164 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:28,165 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  20%|█▍     | 2/10 [00:06<00:27,  3.49s/it]2025-01-17:14:01:31,661 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:31,662 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  30%|██     | 3/10 [00:10<00:24,  3.50s/it]2025-01-17:14:01:35,165 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:35,166 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  40%|██▊    | 4/10 [00:13<00:20,  3.50s/it]2025-01-17:14:01:38,498 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:38,499 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  50%|███▌   | 5/10 [00:17<00:17,  3.44s/it]2025-01-17:14:01:41,956 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:41,956 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  60%|████▏  | 6/10 [00:20<00:13,  3.45s/it]2025-01-17:14:01:45,493 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:45,494 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  70%|████▉  | 7/10 [00:24<00:10,  3.48s/it]2025-01-17:14:01:49,009 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:49,009 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  80%|█████▌ | 8/10 [00:27<00:06,  3.49s/it]2025-01-17:14:01:52,600 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:52,601 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...:  90%|██████▎| 9/10 [00:31<00:03,  3.52s/it]2025-01-17:14:01:56,008 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:01:56,008 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running generate_until function ...: 100%|██████| 10/10 [00:34<00:00,  3.48s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "watsonx_llm (model_id=meta-llama/llama-3-1-8b-instruct), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: 1\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |  0.0|±  |0.0000|\n",
      "|     |       |strict-match    |     5|exact_match|↑  |  0.8|±  |0.1333|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model watsonx_llm \\\n",
    "--verbosity ERROR \\\n",
    "--model_args model_id=meta-llama/llama-3-1-8b-instruct \\\n",
    "--limit 10 \\\n",
    "--tasks gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5176088f-1f12-4df7-bf92-069667aa8dbc"
   },
   "source": [
    "If you get a following error: \n",
    "`RuntimeError: Model [model_id] is not supported: does not return logprobs for input tokens` try again with a different model that has the `logprobs` enabled. Available models can be found [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d2f0c0a-29a4-42dd-872d-fd1402ace391"
   },
   "source": [
    "## Preparing own data for benchmarking <a name=\"prepare-own-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03764960-193c-42f2-8a82-cb75bfdd974b"
   },
   "source": [
    "### Prepare an APIClient instance <a name=\"prepare-client\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "435f594d-4ba5-48c4-a5a6-1f2bf563b40f"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "\n",
    "api_client = APIClient(credentials=Credentials(api_key=api_key, url=url), project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a774c482-5b31-4cf3-bf5a-b68b647ed4cb"
   },
   "source": [
    "### Prepare data assets <a name=\"prepare-assets\"></a>\n",
    "\n",
    "This example uses the datasets from ARC-Easy dataset: \n",
    "*  test-00000-of-00001.parquet\n",
    "*  train-00000-of-00001.parquet\n",
    "*  validation-00000-of-00001.parquet\n",
    "\n",
    "The datasets are available [on HuggingFace](https://huggingface.co/datasets/allenai/ai2_arc/tree/main/ARC-Easy).\n",
    "\n",
    "Let's save their names for further use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ad789f59-fc3d-4c0f-8640-cce7ddba0e87"
   },
   "outputs": [],
   "source": [
    "train_filename = \"train-00000-of-00001.parquet\"\n",
    "test_filename = \"test-00000-of-00001.parquet\"\n",
    "validation_filename = \"validation-00000-of-00001.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf98ba89-70cb-4b78-badf-b23ac7249642"
   },
   "source": [
    "If you are running this notebook locally, you can download the datasets directly from HuggingFace hub or using `wget`. If you already have the files in your desired location or are using DataConnections, skip the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "05b67a23-5eda-4db7-8023-4f2da41aedb4"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "base_url = 'https://huggingface.co/datasets/allenai/ai2_arc/resolve/main/ARC-Easy/'\n",
    "\n",
    "for filename in (train_filename, test_filename, validation_filename): \n",
    "    path = Path(os.getcwd()) / filename\n",
    "    if path.exists():\n",
    "        path.unlink()\n",
    "    wget.download(f\"{base_url}{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca60fc79-a5cb-4d26-ac57-15583809d59e"
   },
   "source": [
    "If you are running this notebook on Cloud and wish to use a connection to a data asset, execute the cells below.\n",
    "\n",
    "__Action__: Provide paths to files that you wish to create data assets with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6bf48b82-58bb-49ae-8ac0-1f1355b4bf80"
   },
   "outputs": [],
   "source": [
    "def create_asset(client, file_path): \n",
    "    asset_details = client.data_assets.create(file_path=file_path, name=Path(file_path).name)\n",
    "    return client.data_assets.get_id(asset_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8aec0adf-26a1-49e2-a928-31c4d0dd6b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data asset...\n",
      "SUCCESS\n",
      "Creating data asset...\n",
      "SUCCESS\n",
      "Creating data asset...\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "train_asset_id = create_asset(api_client, train_filename)\n",
    "test_asset_id = create_asset(api_client, test_filename)\n",
    "validation_asset_id = create_asset(api_client, validation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5c6bfec-7f02-467a-98c0-affbed42983b"
   },
   "source": [
    "If you already have the connections to the following files and wish to download from them, skip the cells above and execute the cell below\n",
    "\n",
    "__Action__: Provide existing connection ID's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0db8e1e-b4e8-4311-9b18-96fc2b2f81db"
   },
   "outputs": [],
   "source": [
    "train_asset_id = \"INSERT TRAIN ASSET ID HERE\"\n",
    "test_asset_id = \"INSERT TEST ASSET ID HERE\"\n",
    "validation_asset_id = \"INSERT VALIDATION ASSET ID HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5eb5c6d-4995-447d-bfa5-86de11212bef"
   },
   "source": [
    "### Download data from Data Assets <a name=\"download-files\"></a>\n",
    "\n",
    "If your files are already stored locally in your desired location, skip the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5dbdcf74-7860-4a90-bc5b-2ce92ec53277"
   },
   "outputs": [],
   "source": [
    "download_file = lambda client, asset_id, name: client.data_assets.download(asset_id=asset_id, filename=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7cb72102-b18c-4aae-a6ce-78623a2b4571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data asset content to file: 'train-00000-of-00001.parquet'\n",
      "Successfully saved data asset content to file: 'test-00000-of-00001.parquet'\n",
      "Successfully saved data asset content to file: 'validation-00000-of-00001.parquet'\n"
     ]
    }
   ],
   "source": [
    "for asset_id, name in zip((train_asset_id, test_asset_id, validation_asset_id), (train_filename, test_filename, validation_filename)):\n",
    "    path = Path(os.getcwd()) / name\n",
    "    if path.exists():\n",
    "        path.unlink()\n",
    "    download_file(api_client, asset_id, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbfb1bf9-be5f-4faf-896b-0743705ba3b6"
   },
   "source": [
    "### Validate files download <a name=\"validate-download\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ce7d5066-955d-44f3-8870-7dd13ca52990"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/wsuser/work/validation-00000-of-00001.parquet',\n",
       " '/home/wsuser/work/train-00000-of-00001.parquet',\n",
       " '/home/wsuser/work/test-00000-of-00001.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str, Path(os.getcwd()).iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13b31aa4-e0e2-4d5b-8f52-c7241cc9849b"
   },
   "source": [
    "### Sample YAML task syntax <a name=\"yaml-task\">\n",
    "For this section we will be using the [`arc_easy`](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/arc) task as an example on how to build a task and execute it from outside of the `lm-evaluation-harness` repository. \n",
    "Tasks for benchmarking are stored as `yaml` files. Let's look at the [Arc-Easy](https://huggingface.co/datasets/allenai/ai2_arc/tree/main/ARC-Easy) dataset and its corresponding task. The `yaml` file containing task info looks like this: \n",
    "\n",
    "```\n",
    "tag:\n",
    "  - ai2_arc\n",
    "task: arc_easy\n",
    "dataset_path: allenai/ai2_arc\n",
    "dataset_name: ARC-Easy\n",
    "output_type: multiple_choice\n",
    "training_split: train\n",
    "validation_split: validation\n",
    "test_split: test\n",
    "doc_to_text: \"Question: {{question}}\\nAnswer:\"\n",
    "doc_to_target: \"{{choices.label.index(answerKey)}}\"\n",
    "doc_to_choice: \"{{choices.text}}\"\n",
    "should_decontaminate: true\n",
    "doc_to_decontamination_query: \"Question: {{question}}\\nAnswer:\"\n",
    "metric_list:\n",
    "  - metric: acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "  - metric: acc_norm\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "metadata:\n",
    "  version: 1.0\n",
    "```\n",
    "\n",
    "Normally, the `dataset_path` and `dataset_name` point to datasets that are stored in `HuggingFace` hub and the `task` points to the list of tasks registered inside the `lm-evaluation-harness` repo. However, it's possible to point to a local dataset with a custom made task. In order to do so, the user need to specify the local paths in the `dataset_kwargs` field and the files type in the `dataset_path` field:\n",
    "\n",
    "```\n",
    "dataset_path: file_type (arrow, parquet, jsonl...)\n",
    "dataset_kwargs:\n",
    "  data_files:\n",
    "    train: /path/to/train/train_file\n",
    "    validation: /path/to/validation/validation_file\n",
    "    test: /path/to/test/test_file\n",
    "```\n",
    "It is also necessary to have the local `yaml` file saved to a specific path and this path needs to be included when calling the `lm-eval` command\n",
    "\n",
    "Knowing what should be included in the task structure we can recreate a dictionary with this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6ed1dc27-814e-4ccf-9f74-d003780aaa7e"
   },
   "outputs": [],
   "source": [
    "task = dict(\n",
    "    tag=\"test_task_ai2_arc_local\",\n",
    "    task=\"test_task_local\",\n",
    "    dataset_path=\"parquet\",\n",
    "    dataset_kwargs={\n",
    "        \"data_files\": {\n",
    "            \"train\": train_filename,\n",
    "            \"test\": test_filename,\n",
    "            \"validation\": validation_filename\n",
    "        }\n",
    "    },\n",
    "    output_type=\"multiple_choice\",\n",
    "    training_split=\"train\",\n",
    "    validation_split=\"validation\",\n",
    "    test_split=\"test\",\n",
    "    doc_to_text=\"Question: {{question}}\\\\nAnswer:\",\n",
    "    doc_to_target=\"{{choices.label.index(answerKey)}}\",\n",
    "    doc_to_choice=\"{{choices.text}}\",\n",
    "    should_decontaminate=True,\n",
    "    doc_to_decontamination_query=\"Question: {{question}}\\\\nAnswer:\",\n",
    "    metric_list=[\n",
    "        {\n",
    "            \"metric\": \"acc\",\n",
    "            \"aggregation\": \"mean\",\n",
    "            \"higher_is_better\": True\n",
    "        },\n",
    "        {\n",
    "            \"metric\": \"acc_norm\",\n",
    "            \"aggregation\": \"mean\",\n",
    "            \"higher_is_better\": True\n",
    "        },\n",
    "    ],\n",
    "    metadata={\"version\": \"1.0\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b563096-84f2-447c-ae95-8922817abc6c"
   },
   "source": [
    "### Save task to file <a name=\"save-task\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0b8463a4-a928-4a79-8ade-e0370c1d0718"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import yaml\n",
    "\n",
    "with codecs.open(\"test_task.yaml\", \"w\") as yaml_file: \n",
    "    yaml.dump(task, yaml_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4881f160-8200-4c18-bf7b-823f85a1c297"
   },
   "source": [
    "## Run `lm_evaluation-harness` benchmarks with local data <a name=\"benchmarks-local\"></a>\n",
    "\n",
    "Having the datasets and the yaml task stored, we can run the lm-eval command with the `--include_path .` argument that will point to the local path and the local `arc_easy` task name (`test_task_local`). Evaluation results will be saved to the specified (`results`) directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "6fb394eb-2ba2-4a94-a692-d30266a46702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-17:14:39:18,643 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2025-01-17:14:39:18,643 INFO     [__main__.py:303] Including path: .\n",
      "2025-01-17:14:39:24,958 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-01-17:14:39:24,959 INFO     [__main__.py:376] Selected Tasks: ['test_task_local']\n",
      "2025-01-17:14:39:24,960 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-01-17:14:39:24,960 INFO     [evaluator.py:201] Initializing watsonx_llm model, with arguments: {'model_id': 'ibm/granite-13b-instruct-v2'}\n",
      "2025-01-17:14:39:25,792 INFO     [client.py:443] Client successfully initialized\n",
      "2025-01-17:14:39:26,199 INFO     [wml_resource.py:112] Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-01-10&project_id=7e8b59ba-2610-4a29-9d90-dc02483ed5f4&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n",
      "2025-01-17:14:39:26,547 INFO     [task.py:415] Building contexts for test_task_local on rank 0...\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 1228.27it/s]\n",
      "2025-01-17:14:39:26,556 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "2025-01-17:14:39:26,898 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:26,899 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:   0%|                | 0/40 [00:00<?, ?it/s]2025-01-17:14:39:26,960 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:26,960 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,051 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,052 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:   2%|▏       | 1/40 [00:00<00:05,  6.57it/s]2025-01-17:14:39:27,115 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,116 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,199 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,199 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:   5%|▍       | 2/40 [00:00<00:05,  6.69it/s]2025-01-17:14:39:27,262 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,262 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,349 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,349 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:   8%|▌       | 3/40 [00:00<00:05,  6.67it/s]2025-01-17:14:39:27,415 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,416 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,501 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,501 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  10%|▊       | 4/40 [00:00<00:05,  6.64it/s]2025-01-17:14:39:27,563 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,563 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,659 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,660 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  12%|█       | 5/40 [00:00<00:05,  6.52it/s]2025-01-17:14:39:27,720 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,721 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,810 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,811 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  15%|█▏      | 6/40 [00:00<00:05,  6.55it/s]2025-01-17:14:39:27,874 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,875 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:27,961 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:27,961 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  18%|█▍      | 7/40 [00:01<00:05,  6.59it/s]2025-01-17:14:39:28,027 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:28,027 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:28,115 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:28,116 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  20%|█▌      | 8/40 [00:01<00:04,  6.54it/s]2025-01-17:14:39:28,179 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:28,180 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:28,268 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:28,268 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  22%|█▊      | 9/40 [00:01<00:04,  6.55it/s]2025-01-17:14:39:29,587 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:29,588 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:29,674 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:29,674 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  25%|█▊     | 10/40 [00:02<00:16,  1.85it/s]2025-01-17:14:39:29,922 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:29,922 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,176 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,176 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  28%|█▉     | 11/40 [00:03<00:15,  1.89it/s]2025-01-17:14:39:30,232 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,232 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,320 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,321 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  30%|██     | 12/40 [00:03<00:11,  2.43it/s]2025-01-17:14:39:30,382 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,382 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,473 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,473 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  32%|██▎    | 13/40 [00:03<00:08,  3.00it/s]2025-01-17:14:39:30,534 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,535 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,625 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,626 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  35%|██▍    | 14/40 [00:03<00:07,  3.59it/s]2025-01-17:14:39:30,687 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,687 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,772 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,772 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  38%|██▋    | 15/40 [00:03<00:05,  4.19it/s]2025-01-17:14:39:30,829 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,829 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:30,914 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,914 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  40%|██▊    | 16/40 [00:04<00:05,  4.77it/s]2025-01-17:14:39:30,974 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:30,974 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:31,058 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,058 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  42%|██▉    | 17/40 [00:04<00:04,  5.27it/s]2025-01-17:14:39:31,284 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,285 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:31,371 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,371 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  45%|███▏   | 18/40 [00:04<00:04,  4.41it/s]2025-01-17:14:39:31,454 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,454 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:31,542 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,542 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  48%|███▎   | 19/40 [00:04<00:04,  4.76it/s]2025-01-17:14:39:31,615 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,615 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:31,698 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,698 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  50%|███▌   | 20/40 [00:04<00:03,  5.16it/s]2025-01-17:14:39:31,758 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,758 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:31,845 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,845 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  52%|███▋   | 21/40 [00:04<00:03,  5.56it/s]2025-01-17:14:39:31,900 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:31,900 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,057 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,057 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  55%|███▊   | 22/40 [00:06<00:08,  2.04it/s]2025-01-17:14:39:33,117 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,117 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,214 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,214 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  57%|████   | 23/40 [00:06<00:06,  2.57it/s]2025-01-17:14:39:33,284 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,284 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,397 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,397 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  60%|████▏  | 24/40 [00:06<00:05,  3.05it/s]2025-01-17:14:39:33,462 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,462 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,551 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,551 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  62%|████▍  | 25/40 [00:06<00:04,  3.63it/s]2025-01-17:14:39:33,627 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,628 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,712 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,713 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  65%|████▌  | 26/40 [00:06<00:03,  4.14it/s]2025-01-17:14:39:33,775 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,776 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:33,871 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,872 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  68%|████▋  | 27/40 [00:06<00:02,  4.62it/s]2025-01-17:14:39:33,942 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:33,942 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:34,024 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,025 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  70%|████▉  | 28/40 [00:07<00:02,  5.06it/s]2025-01-17:14:39:34,093 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,093 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:34,185 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,186 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  72%|█████  | 29/40 [00:07<00:02,  5.36it/s]2025-01-17:14:39:34,452 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,452 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:34,541 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,541 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  75%|█████▎ | 30/40 [00:07<00:02,  4.21it/s]2025-01-17:14:39:34,626 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,626 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:34,716 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,716 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  78%|█████▍ | 31/40 [00:07<00:01,  4.57it/s]2025-01-17:14:39:34,777 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,777 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:34,868 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,868 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  80%|█████▌ | 32/40 [00:07<00:01,  5.04it/s]2025-01-17:14:39:34,935 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:34,936 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:35,028 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:35,028 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  82%|█████▊ | 33/40 [00:08<00:01,  5.35it/s]2025-01-17:14:39:35,091 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:35,092 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:35,179 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:35,179 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  85%|█████▉ | 34/40 [00:08<00:01,  5.67it/s]2025-01-17:14:39:36,293 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,293 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:36,378 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,378 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  88%|██████▏| 35/40 [00:09<00:02,  2.07it/s]2025-01-17:14:39:36,442 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,442 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:36,533 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,533 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  90%|██████▎| 36/40 [00:09<00:01,  2.60it/s]2025-01-17:14:39:36,600 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,600 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:36,690 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,690 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  92%|██████▍| 37/40 [00:09<00:00,  3.16it/s]2025-01-17:14:39:36,756 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,756 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:36,849 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,849 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  95%|██████▋| 38/40 [00:09<00:00,  3.72it/s]2025-01-17:14:39:36,911 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,912 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:36,998 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:36,999 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...:  98%|██████▊| 39/40 [00:10<00:00,  4.29it/s]2025-01-17:14:39:37,269 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:37,270 INFO     [wml_resource.py:112] Successfully finished tokenize for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/tokenization?version=2025-01-10'\n",
      "2025-01-17:14:39:37,359 INFO     [_client.py:1027] HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10 \"HTTP/1.1 200 OK\"\n",
      "2025-01-17:14:39:37,359 INFO     [wml_resource.py:112] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-01-10'\n",
      "Running loglikelihood function ...: 100%|███████| 40/40 [00:10<00:00,  3.82it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2025-01-17:14:39:42,607 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "watsonx_llm (model_id=ibm/granite-13b-instruct-v2), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: 1\n",
      "|     Tasks     |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n",
      "|---------------|------:|------|-----:|--------|---|----:|---|-----:|\n",
      "|test_task_local|      1|none  |     0|acc     |↑  |  0.7|±  |0.1528|\n",
      "|               |       |none  |     0|acc_norm|↑  |  0.6|±  |0.1633|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model watsonx_llm \\\n",
    "--model_args model_id=ibm/granite-13b-instruct-v2 \\\n",
    "--include_path . \\\n",
    "--limit 10 \\\n",
    "--tasks test_task_local \\\n",
    "--output_path results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3183ef7f-cbfc-4b38-a780-3e92406e3296"
   },
   "source": [
    "Now let's see the evaluation results. The file name consists of the `results` prefix and a unique timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "d0795a4a-bb89-4440-81c1-66b8b8ae6263"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_results(file_name): \n",
    "    with open(file_name) as results_file: \n",
    "        return json.loads(results_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "ffbb996f-9236-463b-ac16-7062d6c6ca7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/wsuser/work/results/results_2025-01-17T14-39-42.608162.json']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files = list(map(str, (Path(os.getcwd()) / \"results\").iterdir()))\n",
    "results_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dab7e5c-9fd1-4942-ac4a-2ca08567d463"
   },
   "source": [
    "For pretty printing reasons, the `pretty_env_info` is excluded from output. However, if you want to see this data, comment the `try... except...` block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "1ef3dc31-7165-4f59-86b0-8ba2f5e6c54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": {\n",
      "    \"test_task_local\": {\n",
      "      \"alias\": \"test_task_local\",\n",
      "      \"acc,none\": 0.7,\n",
      "      \"acc_stderr,none\": 0.15275252316519466,\n",
      "      \"acc_norm,none\": 0.6,\n",
      "      \"acc_norm_stderr,none\": 0.16329931618554522\n",
      "    }\n",
      "  },\n",
      "  \"group_subtasks\": {\n",
      "    \"test_task_local\": []\n",
      "  },\n",
      "  \"configs\": {\n",
      "    \"test_task_local\": {\n",
      "      \"task\": \"test_task_local\",\n",
      "      \"tag\": \"test_task_ai2_arc_local\",\n",
      "      \"dataset_path\": \"parquet\",\n",
      "      \"dataset_kwargs\": {\n",
      "        \"data_files\": {\n",
      "          \"test\": \"test-00000-of-00001.parquet\",\n",
      "          \"train\": \"train-00000-of-00001.parquet\",\n",
      "          \"validation\": \"validation-00000-of-00001.parquet\"\n",
      "        }\n",
      "      },\n",
      "      \"training_split\": \"train\",\n",
      "      \"validation_split\": \"validation\",\n",
      "      \"test_split\": \"test\",\n",
      "      \"doc_to_text\": \"Question: {{question}}\\\\nAnswer:\",\n",
      "      \"doc_to_target\": \"{{choices.label.index(answerKey)}}\",\n",
      "      \"doc_to_choice\": \"{{choices.text}}\",\n",
      "      \"description\": \"\",\n",
      "      \"target_delimiter\": \" \",\n",
      "      \"fewshot_delimiter\": \"\\n\\n\",\n",
      "      \"num_fewshot\": 0,\n",
      "      \"metric_list\": [\n",
      "        {\n",
      "          \"aggregation\": \"mean\",\n",
      "          \"higher_is_better\": true,\n",
      "          \"metric\": \"acc\"\n",
      "        },\n",
      "        {\n",
      "          \"aggregation\": \"mean\",\n",
      "          \"higher_is_better\": true,\n",
      "          \"metric\": \"acc_norm\"\n",
      "        }\n",
      "      ],\n",
      "      \"output_type\": \"multiple_choice\",\n",
      "      \"repeats\": 1,\n",
      "      \"should_decontaminate\": true,\n",
      "      \"doc_to_decontamination_query\": \"Question: {{question}}\\\\nAnswer:\",\n",
      "      \"metadata\": {\n",
      "        \"version\": \"1.0\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"versions\": {\n",
      "    \"test_task_local\": \"1.0\"\n",
      "  },\n",
      "  \"n-shot\": {\n",
      "    \"test_task_local\": 0\n",
      "  },\n",
      "  \"higher_is_better\": {\n",
      "    \"test_task_local\": {\n",
      "      \"acc\": true,\n",
      "      \"acc_norm\": true\n",
      "    }\n",
      "  },\n",
      "  \"n-samples\": {\n",
      "    \"test_task_local\": {\n",
      "      \"original\": 2376,\n",
      "      \"effective\": 10\n",
      "    }\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"model\": \"watsonx_llm\",\n",
      "    \"model_args\": \"model_id=ibm/granite-13b-instruct-v2\",\n",
      "    \"batch_size\": 1,\n",
      "    \"batch_sizes\": [],\n",
      "    \"device\": null,\n",
      "    \"use_cache\": null,\n",
      "    \"limit\": 10.0,\n",
      "    \"bootstrap_iters\": 100000,\n",
      "    \"gen_kwargs\": null,\n",
      "    \"random_seed\": 0,\n",
      "    \"numpy_seed\": 1234,\n",
      "    \"torch_seed\": 1234,\n",
      "    \"fewshot_seed\": 1234\n",
      "  },\n",
      "  \"git_hash\": null,\n",
      "  \"date\": 1737124764.9599755,\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"upper_git_hash\": null,\n",
      "  \"task_hashes\": {},\n",
      "  \"model_source\": \"watsonx_llm\",\n",
      "  \"model_name\": \"\",\n",
      "  \"model_name_sanitized\": \"\",\n",
      "  \"system_instruction\": null,\n",
      "  \"system_instruction_sha\": null,\n",
      "  \"fewshot_as_multiturn\": false,\n",
      "  \"chat_template\": null,\n",
      "  \"chat_template_sha\": null,\n",
      "  \"start_time\": 4998773.540018747,\n",
      "  \"end_time\": 4998797.50387703,\n",
      "  \"total_evaluation_time_seconds\": \"23.963858283124864\"\n",
      "}\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for file in results_files:\n",
    "    data = read_json_results(file)\n",
    "    try:\n",
    "        data.pop(\"pretty_env_info\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    print(json.dumps(data, indent=2), end=\"\\n------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7927ccdc-2326-4ee3-afb5-645423485930"
   },
   "source": [
    "## Summary and next steps <a name=\"summary\"></a>\n",
    "You successfully completed this notebook!\n",
    "\n",
    "You learned how to use `ibm-watsonx-ai` and `lm-evaluation-harness` to run custom local and registered benchmarks.\n",
    "\n",
    "Check out our [Online Documentation](https://www.ibm.com/cloud/watson-studio/autoai) for more samples, tutorials, documentation, how-tos, and blog posts.\n",
    "\n",
    "### Authors \n",
    "<b>Marta Tomzik</b>, Software Engineer at Watson Machine Learning.\n",
    "\n",
    "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
