{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx, `llama-3-1-70b-instruct` and LlamaIndex to make simple chat conversation and tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook provides a detailed demonstration of the steps and code required to showcase support for Chat models, including the integration of tools using [LlamaIndex](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/), ReActAgent and watsonx.ai models.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The purpose of this notebook is to show how to use chat models like `meta-llama/llama-3-1-70b-instruct` using the LlamaIndex tools and integration with ReActAgent.\n",
    "\n",
    "LlamaIndex is an open source data orchestration framework for building large language model (LLM) applications. LlamaIndex is available in Python and TypeScript and leverages a combination of tools and capabilities that simplify the process of context augmentation for generative AI (gen AI) use cases through a Retrieval-Augmented (RAG) pipeline. \n",
    "\n",
    "More examples can be found [here](https://docs.llamaindex.ai/en/stable/examples/llm/ibm_watsonx/).\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [LlamaIndex integration](#chatwatsonx)\n",
    "- [Using ReActAgent for chatting](#reactagent)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the `datasets` and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U \"llama-index-llms-ibm>=0.3.0\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the WML credentials\n",
    "Use the code cell below to define the WML credentials that are required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">Managing user API keys</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key=getpass.getpass(\"Enter your WML API key and hit enter: \"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the project ID\n",
    "You need to provide the project ID to give the Foundation Model the context for the call. If you have a default project ID set in Watson Studio, the notebook obtains that project ID. Otherwise, you need to provide the project ID in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Enter your project_id and hit enter: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "## Set up a Foundation Model on `watsonx.ai`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `model_id` of the model you will use for the chat with tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/llama-3-1-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"chatwatsonx\"></a>\n",
    "## LlamaIndex integration\n",
    "\n",
    "`WatsonxLLM` is a wrapper around watsonx.ai models that provides chat integration around these models.\n",
    "\n",
    "### Initialize the `WatsonxLLM` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=credentials.url,\n",
    "    apikey=credentials.api_key,\n",
    "    project_id=project_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer a simple question using a defined object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Generative AI refers to artificial intelligence technologies that can generate new content, such as text, images, or audio, based on patterns and structures learned from existing data.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "msg = ChatMessage(role=MessageRole.USER, content='Answer in short sentence: what is generative AI?')\n",
    "print(llm.chat([msg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To drive a car, start the engine, check your mirrors and surroundings, release the brake, and slowly press the accelerator while steering in your desired direction."
     ]
    }
   ],
   "source": [
    "msg = ChatMessage(role=MessageRole.USER, content='Answer in short sentence: how to drive a car?')\n",
    "for x in llm.stream_chat([msg]):\n",
    "    print(x.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reactagent\"></a>\n",
    "## Use ReActAgent for chatting\n",
    "\n",
    "Let's define the assistant tools for calculations and the `ReActAgent` object for chatting and streaming.\n",
    "\n",
    "More details about `ReActAgent` itself [here](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two floats and returns the result float\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract a and b.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide a and b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)\n",
    "\n",
    "tools = [add_tool, subtract_tool, multiply_tool, divide_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step f25abc83-78c7-4f62-b2e5-c66e0ba4b066. Step input: What is 20 + (2 * 4)? Calculate step by step \n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me calculate the expression 2 * 4.\n",
      "Action: multiply\n",
      "Action Input: {'a': 2, 'b': 4}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 8\n",
      "\u001b[0m> Running step 99cd4dad-1cf3-45f0-8587-d9f851b984a5. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Now that I have the result of the multiplication, I can proceed with the addition.\n",
      "Action: add\n",
      "Action Input: {'a': 20, 'b': 8}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 28\n",
      "\u001b[0m> Running step b07c5c9e-d55f-437a-bfda-877fb544b1d3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have now calculated the entire expression 20 + (2 * 4).\n",
      "Answer: 28\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is 20 + (2 * 4)? Calculate step by step \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chat history\n",
    "\n",
    "Have a conversation with your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 73ea7cca-22d5-4e48-9494-3b6f26c3c6b3. Step input: Who are you?\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step dd73b261-f418-4304-a3c8-86c861f5e704. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to introduce myself as a pirate.\n",
      "Answer: Me name be Captain Jack Blackbeak, the most feared pirate to ever sail the Seven Seas.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=MessageRole.USER, content=\"You are a pirate!\"), \n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=\"Amma pirate! My name is Jack Blackbeak.\")\n",
    "]\n",
    "\n",
    "response = agent.chat('Who are you?', chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chat history and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step de928379-a0ae-4c74-a343-221675a661fa. Step input: Currently we have 2024. When I was born?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "\n",
      "To find the birth year, we can subtract the age from the current year. However, the tool 'subtract' requires two numbers to subtract. We can use the given age and the current year as the two numbers.\n",
      "Action: subtract\n",
      "Action Input: {'a': 2024, 'b': 45}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 1979\n",
      "\u001b[0m> Running step a0d00f41-29be-44a4-906b-b9e03aec38f6. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: You were born in 1979.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "msg = ChatMessage(role=MessageRole.USER, \n",
    "                  content=f\"I was born in Nevada, {45} years ago. I am AI engineer\")\n",
    "\n",
    "response = agent.chat(f'Currently we have {2024}. When I was born?', chat_history=[msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 14d7baa9-7652-4bb3-803c-b5526498ff01. Step input: What is (20 / 5) + (2 * 4)? Calculate step by step \n",
      "\u001b[1;3;38;5;200mThought: To calculate (20 / 5) + (2 * 4), I need to use the tools to perform division, multiplication and addition. I will start with division.\n",
      "Action: divide\n",
      "Action Input: {'a': 20, 'b': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 4.0\n",
      "\u001b[0m> Running step 1d50fc9a-d45a-41ee-8357-4ad3a46baa01. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Now that I have the result of the division, I need to calculate the multiplication part of the expression, which is 2 * 4.\n",
      "Action: multiply\n",
      "Action Input: {'a': 2, 'b': 4}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 8\n",
      "\u001b[0m> Running step ea5bccef-15ad-43ea-b7ae-9a8defbb469b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Now that I have the results of both the division and the multiplication, I can calculate the final result by adding them together.\n",
      "Action: add\n",
      "Action Input: {'a': 4.0, 'b': 8}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 12.0\n",
      "\u001b[0m> Running step 583a8edc-4061-4ca2-8248-58486c84b8a5. Step input: None\n"
     ]
    }
   ],
   "source": [
    "response = agent.stream_chat(\"What is (20 / 5) + (2 * 4)? Calculate step by step \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the generator, the user can simply call the `response.print_response_stream()` function or consume the generator via the `response_gen` field from the `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The result of (20 / 5) + (2 * 4) is 12.0."
     ]
    }
   ],
   "source": [
    "for x in response.response_gen:\n",
    "    print(x, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    "You successfully completed this notebook!\n",
    "\n",
    "You learned how to build a simple agent using ReActAgent and `WatsonLLM`.\n",
    "\n",
    "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Author\n",
    "\n",
    "**Wojciech Rębisz**, Software Engineer at Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
