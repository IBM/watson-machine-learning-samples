{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx.ai and LangChain Agents to perform sequence of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate usage of LangChain Agents with watsonx.ai models.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to use language models, e.g. `meta-llama/llama-3-70b-instruct`, to create Structured Chat LangChain Agent with memory, where an Agent may perform sequence of actions based on the model responses and assures that LLM uses chat memory to provide the most accurate answer on the user question.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [LangChain integration](#watsonxllm)\n",
    "- [Prompt template and custom tools](#prompt_template)\n",
    "- [LangChain Agent](#agent)\n",
    "- [Example of Structured Chat](#chat)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-service-instance.html?context=analytics\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the `datasets` and dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U ibm-watsonx-ai | tail -n 1\n",
    "!pip install langchain | tail -n 1\n",
    "!pip install langchain-community | tail -n 1\n",
    "!pip install langchain-ibm | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the WML credentials\n",
    "This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key=getpass.getpass(\"Please enter your WML api key (hit enter): \"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the project id\n",
    "The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"watsonxllm\"></a>\n",
    "## Initialize `WatsonxLLM` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set model parameters\n",
    "\n",
    "To set the model parameters we use `GenTextParamsMetaNames` metanames from `ibm-watsonx-ai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performed experiments we found that the following parameters work well with the LangChain Agents created in the further section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 200,\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 10,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.1,\n",
    "    GenTextParamsMetaNames.TOP_K: 20,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you need to specify `model_id` that will be used for inferencing. In the following notebook we use `meta-llama/llama-3-70b-instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = WatsonxLLM(url=credentials.get('url'),\n",
    "                      apikey=credentials.get('apikey'),\n",
    "                      project_id=project_id,\n",
    "                      model_id=\"meta-llama/llama-3-70b-instruct\",\n",
    "                      params=parameters\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generative AI is a type of artificial intelligence that can create new, original content, such as images, videos, music, or text, based on patterns and structures it has learned from existing data.\n"
     ]
    }
   ],
   "source": [
    "response = llm_chat.invoke('What is a generative AI? Answer briefly.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prompt_template\"></a>\n",
    "## Prompt template preparation and tools\n",
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A structured chat prompt template is composed of several sections: system prompt, chat history and user input. The following prompt template was prepared based on the LangChain [documentation](https://python.langchain.com/docs/modules/agents/agent_types/structured_chat) and adjusted to work with the chat models available on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant prioritizes caution over usefulness, refusing to answer questions that it considers unsafe, immoral, unethical or dangerous. \n",
    "\n",
    "AI assistant has access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a JSON blob to specify a tool by providing an action key (tool name) and an action input key (tool input). \n",
    "\n",
    "Use only valid \"action\" values: \"Final Answer\" or {tool_names} and provide only one action per $JSON_BLOB. Use \"action\" value \"Final Answer\" if you know what to respond. Provide only ONE action per $JSON_BLOB, as shown:\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools only if necessary. Format is Action:```$JSON_BLOB```then Observation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '{input}\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what and use tools only if necessary)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final structured chat prompt template one can use initialization class method from `ChatPromptTemplate` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", system_prompt\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        (\"user\", user_input),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], template='Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant prioritizes caution over usefulness, refusing to answer questions that it considers unsafe, immoral, unethical or dangerous. \\n\\nAI assistant has access to the following tools:\\n\\n{tools}\\n\\nUse a JSON blob to specify a tool by providing an action key (tool name) and an action input key (tool input). \\n\\nUse only valid \"action\" values: \"Final Answer\" or {tool_names} and provide only one action per $JSON_BLOB. Use \"action\" value \"Final Answer\" if you know what to respond. Provide only ONE action per $JSON_BLOB, as shown:\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\nFollow this format:\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\nReminder to ALWAYS respond with a valid JSON blob of a single action. Use tools only if necessary. Format is Action:```$JSON_BLOB```then Observation\\n')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what and use tools only if necessary)'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we prepare tools (python functions) that the AI assistant will have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.tools import StructuredTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of the given word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove all punctuation symbols from the text.\"\"\"\n",
    "    import re \n",
    "\n",
    "    new_string = re.sub(r'[^\\w\\s]','',text) \n",
    "    return new_string\n",
    "\n",
    "@tool\n",
    "def get_mean_from_sequence(sequence: str) -> float:\n",
    "    \"\"\"Return the mean value from the sequence of numbers.\"\"\"\n",
    "    import numpy\n",
    "    return numpy.mean(numpy.array(sequence.split(','), dtype=float))\n",
    "\n",
    "## Structured tools\n",
    "\n",
    "def get_normalize_vector(vector: list[float]) -> str:\n",
    "    \"\"\"Return a normalized `vector`.\"\"\"\n",
    "    import numpy\n",
    "    return str(list(vector / numpy.sqrt(numpy.sum(numpy.array(vector)**2))))\n",
    "\n",
    "get_normalize_vector_structured = StructuredTool.from_function(\n",
    "    func=get_normalize_vector,\n",
    "    name=\"Normalize vector\",\n",
    "    description=\"useful when want to normalize a vector\",\n",
    ")\n",
    "\n",
    "def get_gcd(a: int, b: int) -> int:\n",
    "    \"\"\"Return the greatest common divisor of the two integers.\"\"\"\n",
    "    import math\n",
    "    return math.gcd(a, b)\n",
    "\n",
    "get_gcd_structured = StructuredTool.from_function(\n",
    "    func=get_gcd,\n",
    "    name=\"Greatest common divisor\",\n",
    "    description=\"useful when want to find greatest common divisor of the two integers\",\n",
    ")\n",
    "\n",
    "tools_chat = [get_word_length, get_mean_from_sequence, remove_punctuations, get_normalize_vector_structured, get_gcd_structured]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"agent\"></a>\n",
    "## Initialize an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following section is the central part of the presented notebook. Here, we construct LangChain Agent and initialise `AgentExecutor` which manage conversation with AI assistant, that has access to the predefined tools, and human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prompt template, we include the tools signature and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chat = prompt.partial(\n",
    "        tools=[str(convert_to_openai_tool(func)) \n",
    "               if isinstance(func, StructuredTool) \n",
    "               else render_text_description([func]) \n",
    "               for func in tools_chat],\n",
    "        tool_names=\", \".join([t.name for t in tools_chat]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we have all components prepared we can define Chat Agent and initialize `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop sequence for LLM\n",
    "llm_with_stop = llm_chat.bind(stop=[\"Observation:\"])\n",
    "\n",
    "agent_chat = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    )\n",
    "    | prompt_chat\n",
    "    | llm_with_stop\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor_chat = AgentExecutor(\n",
    "    agent=agent_chat, tools=tools_chat, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use `handle_parsing_errors=True` to ensure that when LLM is not able to determine what step to take because its output does not have the correct structure to be handled  by output parser, the user's input will be sent back to the model with the appropriate information to improve the model reponse. Setitng this parameter to True help to keep chat conversation smooth, but may also result in multiple requests with many tokens being sent to the model and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chat\"></a>\n",
    "## Structured Chat with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include chat memory in conversation with AI assistant we are going to use `ChatMessageHistory` that simply store chat history in the prompt and send it with every user question to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor_chat,\n",
    "    get_session_history=lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `session_id` is needed to persist conversation histories. Note that watsonx.ai does not conversation history. Let us start the chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "\n",
      "Question: Hi! How are you?\n",
      "Thought: I've seen this question before. I should respond politely.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I'm doing well, thank you for asking!\"\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you for asking!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Hi! How are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "\n",
      "Question: Normalize the following vector: `1, 2, 4`.\n",
      "Thought: I need to normalize the vector.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Normalize vector\",\n",
      "  \"action_input\": {\"vector\": [1, 2, 4]}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m[0.2182178902359924, 0.4364357804719848, 0.8728715609439696]\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The normalized vector is [0.2182178902359924, 0.4364357804719848, 0.8728715609439696].\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The normalized vector is [0.2182178902359924, 0.4364357804719848, 0.8728715609439696].'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Normalize the following vector: `1, 2, 4`.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "Question: What is the greatest common divisor of integers 1215 and 645?\n",
      "Thought: I need to use the Greatest common divisor tool\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Greatest common divisor\",\n",
      "  \"action_input\": {\"a\": 1215, \"b\": 645}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[33;1m\u001b[1;3m15\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The greatest common divisor of integers 1215 and 645 is 15.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The greatest common divisor of integers 1215 and 645 is 15.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the greatest common divisor of integers 1215 and 645?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2dded844-5d50-4621-9b28-6edbb8e5dc09 not found for run 049d302f-39de-4728-a3e5-03519d49f06f. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "\n",
      "Question: What is a median?\n",
      "Thought: I don't know the answer, but I can try to explain it in simple terms.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"A median is a middle value in a sequence of numbers, where half the numbers are above it and half are below it. However, I'm not sure how to calculate it. Can you provide more context or clarify what you mean?\"\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A median is a middle value in a sequence of numbers, where half the numbers are above it and half are below it. However, I'm not sure how to calculate it. Can you provide more context or clarify what you mean?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is a median?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 72101dec-1497-4b92-9034-033a19d0fee8 not found for run b7852e6d-b956-4a32-afc4-b1e2254b7247. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "\n",
      "Question: Could you be more specific and provide some example?\n",
      "Thought: I need to provide a clear explanation with an example\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_mean_from_sequence\",\n",
      "  \"action_input\": {\"sequence\": \"1, 2, 3, 4, 5\"}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[33;1m\u001b[1;3m3.0\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"To calculate the median, we need to arrange the numbers in order. If we have an odd number of values, the middle value is the median. If we have an even number of values, the median is the average of the two middle values. For example, in the sequence 1, 2, 3, 4, 5, the median is 3, since it's the middle value. In the sequence 1, 2, 3, 4, the median is (2 + 3) / 2 = 2.5, since it's the average of the two middle values.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To calculate the median, we need to arrange the numbers in order. If we have an odd number of values, the middle value is the median. If we have an even number of values, the median is the average of the two middle values. For example, in the sequence 1, 2, 3, 4, 5, the median is 3, since it's the middle value. In the sequence 1, 2, 3, 4, the median is (2 + 3) / 2 = 2.5, since it's the average of the two middle values.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Could you be more specific and provide some example?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use other tools that were provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run f5361457-fe83-4dc1-b1c7-1a3f7526f149 not found for run 949b4fc4-c133-455a-909c-712e0388bde3. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "Question: Use one of the tool and remove all punctuation symbols from the sentence: `What, is a. loan?!`. Return clear sentence.\n",
      "Thought: I can use the remove_punctuations tool to remove all punctuation symbols from the sentence.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"remove_punctuations\",\n",
      "  \"action_input\": {\"text\": \"What, is a. loan?!\"}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[38;5;200m\u001b[1;3mWhat is a loan\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"What is a loan\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is a loan'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Use one of the tool and remove all punctuation symbols from the sentence: `What, is a. loan?!`. Return clear sentence.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "Question: What is the average length of words `watsonx.ai` and `IBM`?\n",
      "Thought: I need to calculate the length of each word and then find the mean.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": {\"word\": \"watsonx.ai\"}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m10\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": {\"word\": \"IBM\"}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m3\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_mean_from_sequence\",\n",
      "  \"action_input\": {\"sequence\": \"10,3\"}\n",
      "}\n",
      "```\n",
      "Observation:\u001b[0m\u001b[33;1m\u001b[1;3m6.5\u001b[0m\u001b[32;1m\u001b[1;3m \n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The average length of words `watsonx.ai` and `IBM` is 6.5.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average length of words `watsonx.ai` and `IBM` is 6.5.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the average length of words `watsonx.ai` and `IBM`?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")\n",
    "response['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of the chat is stored locally in `message_history` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi! How are you?'),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       " HumanMessage(content='Hi! How are you?'),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       " HumanMessage(content='Normalize the following vector: `1, 2, 4`.'),\n",
       " AIMessage(content='The normalized vector is [0.2182178902359924, 0.4364357804719848, 0.8728715609439696].'),\n",
       " HumanMessage(content='What is the greatest common divisor of integers 1215 and 645?'),\n",
       " AIMessage(content='The greatest common divisor of integers 1215 and 645 is 15.'),\n",
       " HumanMessage(content='What is the average length of words `watsonx.ai` and `IBM`?'),\n",
       " AIMessage(content='The average length of words `watsonx.ai` and `IBM` is 6.5.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    " You successfully completed this notebook!.\n",
    " \n",
    " You learned how to use LangChain Agents using custom llm WastonxLLM.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watson-machine-learning-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authors:\n",
    " **Mateusz Świtała**, Software Engineer at Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright © 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
