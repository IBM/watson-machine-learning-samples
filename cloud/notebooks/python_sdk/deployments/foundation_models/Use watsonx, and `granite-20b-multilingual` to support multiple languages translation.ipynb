{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx, and `granite-20b-multilingual` to support multiple languages translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate support for language translation in watsonx. It introduces commands for defining prompt and model testing.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "#### About IBM `granite-20b-multilingual` model\n",
    "\n",
    "The Granite 20 Billion Multilingual (granite-20b-multilingual) model has been trained using over 2.6 trillion tokens and further fine-tuned using a collection of instruction-tuning datasets. The model underwent extensive pre-training utilizing multilingual common crawl data, enabling it to effectively handle the following languages:\n",
    "- **English**, \n",
    "- **German**, \n",
    "- **Spanish**, \n",
    "- **French**,\n",
    "- **Portuguese**.\n",
    "\n",
    "The table below lists the <a href=\"https://github.com/nlp-uoregon/mlmm-evaluation\" target=\"_blank\" rel=\"xMMLU\">xMMLU</a> and <a href=\"https://arxiv.org/pdf/2306.05685.pdf\" target=\"_blank\" rel=\"xMT-Bench\">xMT-Bench</a> benchmarks used to show the performance in 5 languages.\n",
    "\n",
    "| Benchmark | Average | English | German | Spanish | French | Portuguese |\n",
    "|:---------:|:-------:|:-------:|:------:|:-------:|:------:|:----------:|\n",
    "| xMMLU     | 38.41   | 40.58   | 37.91  | 38.04   | 37.58  | 37.95      |\n",
    "| xMT-Bench | 5.34    | 5.59    | 5.18   | 5.17    | 5.19   | 5.58       |\n",
    "\n",
    "\n",
    "For additional information about the `granite-20b-multilingual` model, please refer to the provided <a href=\"https://dataplatform.cloud.ibm.com/wx/samples/models/ibm/granite-20b-multilingual?context=wx\" target=\"_blank\" rel=\"link\"> link.</a>\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The goal of this notebook is to demonstrate how to translate multiple languages using IBM `granite-20b-multilingual` watsonx model based on query provided by the user.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Translate the text based on the query](#translate)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U ibm-watsonx-ai | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the WML credentials\n",
    "This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key=getpass.getpass(\"Please enter your WML api key (hit enter): \"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the project id\n",
    "The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "## Foundation Models on `watsonx.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List available models\n",
    "\n",
    "All available models are listed within the <a href=\"https://ibm.github.io/watsonx-ai-python-sdk/fm_helpers.html#ibm_watsonx_ai.foundation_models.get_model_specs\" target=\"_blank\" rel=\"get_model_specs\">`get_model_specs`</a> function.\n",
    "\n",
    "Additionally, models can be passed as `ModelTypes`. For further details, please consult the <a href=\"https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html#ibm_watsonx_ai.foundation_models.utils.enums.ModelTypes\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bigcode/starcoder',\n",
       " 'bigscience/mt0-xxl',\n",
       " 'codellama/codellama-34b-instruct-hf',\n",
       " 'eleutherai/gpt-neox-20b',\n",
       " 'google/flan-t5-xl',\n",
       " 'google/flan-t5-xxl',\n",
       " 'google/flan-ul2',\n",
       " 'ibm-mistralai/mixtral-8x7b-instruct-v01-q',\n",
       " 'ibm/granite-13b-chat-v1',\n",
       " 'ibm/granite-13b-chat-v2',\n",
       " 'ibm/granite-13b-instruct-v1',\n",
       " 'ibm/granite-13b-instruct-v2',\n",
       " 'ibm/granite-20b-multilingual',\n",
       " 'ibm/mpt-7b-instruct2',\n",
       " 'meta-llama/llama-2-13b-chat',\n",
       " 'meta-llama/llama-2-70b-chat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models import get_model_specs\n",
    "\n",
    "[model[\"model_id\"] for model in get_model_specs(credentials[\"url\"])[\"resources\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to specify `model_id` that will be used for inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm/granite-20b-multilingual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model parameters\n",
    "\n",
    "You might need to adjust model `parameters` for different models or tasks, to do so please refer to <a href=\"https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html#metanames.GenTextParamsMetaNames\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.STOP_SEQUENCES: [\"\\n\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Delete `GenParams.STOP_SEQUENCES: [\"\\n\"]` parameter if you intend to utilize the model for multiple translations on one prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "Initialize the `Model` class with previous set params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id, \n",
    "    params=parameters, \n",
    "    credentials=credentials,\n",
    "    project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-20b-multilingual',\n",
       " 'label': 'granite-20b-multilingual',\n",
       " 'provider': 'IBM',\n",
       " 'source': 'IBM',\n",
       " 'short_description': 'The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.',\n",
       " 'long_description': 'Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.',\n",
       " 'tier': 'class_1',\n",
       " 'number_params': '20b',\n",
       " 'min_shot_size': 1,\n",
       " 'task_ids': ['question_answering',\n",
       "  'summarization',\n",
       "  'retrieval_augmented_generation',\n",
       "  'classification',\n",
       "  'generation',\n",
       "  'extraction'],\n",
       " 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 3}},\n",
       "  {'id': 'summarization', 'ratings': {'quality': 4}},\n",
       "  {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 3}},\n",
       "  {'id': 'classification', 'ratings': {'quality': 4}},\n",
       "  {'id': 'generation'},\n",
       "  {'id': 'extraction', 'ratings': {'quality': 4}}],\n",
       " 'model_limits': {'max_sequence_length': 8192},\n",
       " 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 4096},\n",
       "  'v2-professional': {'call_time': '5m0s', 'max_output_tokens': 4096},\n",
       "  'v2-standard': {'call_time': '5m0s', 'max_output_tokens': 4096}},\n",
       " 'lifecycle': [{'id': 'available', 'start_date': '2024-03-14'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"translate\"></a>\n",
    "\n",
    "## Translate the text based on the query\n",
    "\n",
    "### English to Spanish translation:\n",
    "\n",
    "Define query for the model with at least one example, specifically for English to Spanish translation.\n",
    "\n",
    "**Note:** Model works the best with at least one translation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_to_spanish_query = \"\"\"Translate the following text from English to Spanish:\n",
    "\n",
    "Input: So far, I have not been terribly encouraged by the stance adopted by the Commission.\n",
    "Output: Hasta ahora no me ha animado mucho la postura adoptada por la Comisión.\n",
    "\n",
    "Input: I am very pleased to see that the joint resolution adopts the suggestion we made.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** ensure that there is a line break (newline) at the conclusion of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generat the English to Spanish translation using IBM `granite-20b-multilingual` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_result = model.generate_text(english_to_spanish_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Estoy muy contento de ver que la resolución conjunta adopta la sugerencia que hicimos.\n"
     ]
    }
   ],
   "source": [
    "print(translation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English to French translation:\n",
    "\n",
    "Define query for the model with at least one example, specifically for English to French translation.\n",
    "\n",
    "**Note:** Model works the best with at least one translation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_to_french_query = \"\"\"Translate the following text from English to French:\n",
    "\n",
    "Input: Finally, I welcome paragraph 16 which calls for a review of the way we deal with human rights issues in Parliament.\n",
    "Output: Enfin, je me réjouis du paragraphe 16 qui appelle à une révision de la manière dont nous abordons les questions relatives aux droits de l'homme au sein du Parlement.\n",
    "\n",
    "Input: I remember very well that we discussed it in a session in Luxembourg.\n",
    "Output: Je me souviens très bien que nous en avions parlé lors d'une séance à Luxembourg.\n",
    "\n",
    "Input: If we do not greatly increase the use of intelligent technology, we will not achieve our targets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** ensure that there is a line break (newline) at the conclusion of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generat the English to French translation using IBM `granite-20b-multilingual` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_result = model.generate_text(english_to_french_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Si nous ne faisons pas un usage beaucoup plus important de la technologie intelligente, nous ne parviendrons pas à atteindre nos objectifs.\n"
     ]
    }
   ],
   "source": [
    "print(translation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    " You successfully completed this notebook!.\n",
    " \n",
    " You learned how to translate multiple languages with IBM `granite-20b-multilingual` model on watsonx. \n",
    " \n",
    " Check out our _<a href=\"https://ibm.github.io/watson-machine-learning-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authors: \n",
    "\n",
    " **Mateusz Szewczyk**, Software Engineer at Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright © 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
